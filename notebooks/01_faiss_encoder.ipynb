{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Encoder+FAISS\n",
    "Embeddings are vector representations of words or sentences. They are used in LLM’s to represent language in a format that the model can learn from and understand. Since they are vectors that hold contextual information, many interesting operations can be made with them.\n",
    "\n",
    "For instance, in this notebook I’ve encoded a set of 5 events and 5 queries about these events into embeddings using the lightweight all-MiniLM encoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = [\n",
    "    \"The player entered the dark cave.\",\n",
    "    \"The dragon sleeps on a pile of gold.\",\n",
    "    \"The merchant offered a healing potion.\",\n",
    "    \"The knight swore loyalty to the kings.\",\n",
    "    \"The village was attacked by bandits.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeding shape: (5, 384)\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(events, convert_to_numpy=True, normalize_embeddings=True)\n",
    "print(\"Embeding shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need a way to store these vectors and query them to do interesting things. FAISS is a database which allows for fast index search, instead of looping through manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors indexed: 5\n"
     ]
    }
   ],
   "source": [
    "d = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(d)\n",
    "index.add(embeddings)\n",
    "print(\"Number of vectors indexed:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since they are vectors, I can compare them using FAISS to find shortest euclidian distance between them and find which event has an answer to which query, because similar sentences will be close to each other in this vector space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who attacked the town?\n",
      "Match 1: The village was attacked by bandits. (cosine similarity=0.68)\n",
      "Match 2: The player entered the dark cave. (cosine similarity=0.23)\n",
      "Query: Who owns the treasure?\n",
      "Match 1: The player entered the dark cave. (cosine similarity=0.24)\n",
      "Match 2: The knight swore loyalty to the kings. (cosine similarity=0.19)\n",
      "Query: Where did the player go?\n",
      "Match 1: The player entered the dark cave. (cosine similarity=0.43)\n",
      "Match 2: The merchant offered a healing potion. (cosine similarity=0.20)\n",
      "Query: Who helps with healing?\n",
      "Match 1: The merchant offered a healing potion. (cosine similarity=0.53)\n",
      "Match 2: The dragon sleeps on a pile of gold. (cosine similarity=0.11)\n",
      "Query: Who did the knight swore loyalty to?\n",
      "Match 1: The knight swore loyalty to the kings. (cosine similarity=0.87)\n",
      "Match 2: The merchant offered a healing potion. (cosine similarity=0.24)\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"Who attacked the town?\",\n",
    "    \"Who owns the treasure?\",\n",
    "    \"Where did the player go?\",\n",
    "    \"Who helps with healing?\",\n",
    "    \"Who did the knight swore loyalty to?\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "  query_vec = model.encode([q], convert_to_numpy=True)\n",
    "  D, I = index.search(query_vec, k=2)\n",
    "  print(\"Query:\", q)\n",
    "  for rank, idx in enumerate(I[0]):\n",
    "    print(f\"Match {rank+1}: {events[idx]} (cosine similarity={D[0][rank]:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the results are not that bad! One thing I noticed, in the dragon-treasure query, the model failed. That's a clear indication that this simple setup just captures surface similarity, not reason or inference. Though, it is still impressive as all of the converting, lookup, attention, and pooling happens inside just one line of model.encode\n",
    "\n",
    "In conclusion, we translated the sentences into mathematically represented vector space, which is pretty awesome!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
