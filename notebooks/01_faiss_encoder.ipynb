{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Encoder+FAISS\n",
    "Embeddings are vector representations of words or sentences. They are used in LLM’s to represent language in a format that the model can learn from and understand. Since they are vectors that hold contextual information, many interesting operations can be made with them.\n",
    "\n",
    "For instance, in this notebook I’ve encoded a set of 5 events and 5 queries about these events into embeddings using the lightweight all-MiniLM encoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = [\n",
    "    \"The player entered the dark cave.\",\n",
    "    \"The dragon sleeps on a pile of gold.\",\n",
    "    \"The merchant offered a healing potion.\",\n",
    "    \"The knight swore loyalty to the kings.\",\n",
    "    \"The village was attacked by bandits.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53bb4d244e2b4fa0971bffac048e8f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa09a6bf28044e580ce55fe8c84e95a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8a7c31040c43b3b4bc4086da0212fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdfcd7fe77f3449abda999974b4d63ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca442751a364965a0b9b45e0687e723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bcc155aa47440fe8726537b6f7c9588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f5b571671346c5b77dc8861e685ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b04132cbc848a18d8c5b23d4262a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439613bd2f934e918cdbae378eb1c201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9fe8dca14464f7fabd4bf7f3e687777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d1720830474aeca62ddf86f4fd96da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeding shape: (5, 384)\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(events, convert_to_numpy=True)\n",
    "print(\"Embeding shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need a way to store these vectors and query them to do interesting things. FAISS is a database which allows for fast index search, instead of looping through manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors indexed: 5\n"
     ]
    }
   ],
   "source": [
    "d = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(embeddings)\n",
    "print(\"Number of vectors indexed:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since they are vectors, I can compare them using FAISS to find shortest euclidian distance between them and find which event has an answer to which query, because similar sentences will be close to each other in this vector space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who attacked the town?\n",
      "Match 1: The village was attacked by bandits. (distance=0.65)\n",
      "Match 2: The player entered the dark cave. (distance=1.54)\n",
      "Query: Who owns the treasure?\n",
      "Match 1: The player entered the dark cave. (distance=1.52)\n",
      "Match 2: The knight swore loyalty to the kings. (distance=1.61)\n",
      "Query: Where did the player go?\n",
      "Match 1: The player entered the dark cave. (distance=1.15)\n",
      "Match 2: The merchant offered a healing potion. (distance=1.60)\n",
      "Query: Who helps with healing?\n",
      "Match 1: The merchant offered a healing potion. (distance=0.94)\n",
      "Match 2: The dragon sleeps on a pile of gold. (distance=1.79)\n",
      "Query: Who did the knight swore loyalty to?\n",
      "Match 1: The knight swore loyalty to the kings. (distance=0.26)\n",
      "Match 2: The merchant offered a healing potion. (distance=1.52)\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"Who attacked the town?\",\n",
    "    \"Who owns the treasure?\",\n",
    "    \"Where did the player go?\",\n",
    "    \"Who helps with healing?\",\n",
    "    \"Who did the knight swore loyalty to?\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "  query_vec = model.encode([q], convert_to_numpy=True)\n",
    "  D, I = index.search(query_vec, k=2)\n",
    "  print(\"Query:\", q)\n",
    "  for rank, idx in enumerate(I[0]):\n",
    "    print(f\"Match {rank+1}: {events[idx]} (distance={D[0][rank]:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the results are not that bad! One thing I noticed, in the dragon-treasure query, the model failed. That's a clear indication that this simple setup just captures surface similarity, not reason or inference. Though, it is still impressive as all of the converting, lookup, attention, and pooling happens inside just one line of model.encode\n",
    "\n",
    "In conclusion, we translated the sentences into mathematically represented vector space, which is pretty awesome!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
